{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition for Smart TVs\n",
    "\n",
    "In this project, a neural network will be built to identify 5 different hand gestures performed by the user in order to control a specific function of a Smart TV.\n",
    "\n",
    "**Libraries Used:** Numpy, Scipy, Keras, TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Data Preparation\n",
    "3. Model Building\n",
    "4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Understanding the Business Problem\n",
    "\n",
    "A home electronics company manufactures state of the art smart televisions. They want to develop a new feature where the TV can recognize 5 different hand gestures that a user can perform in order to control the TV without the use of a remote. The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "**Thumbs up:**  Increase the volume <br>\n",
    "**Thumbs down:** Decrease the volume <br>\n",
    "**Left swipe:** 'Jump' backwards 10 seconds <br>\n",
    "**Right swipe:** 'Jump' forward 10 seconds <br>\n",
    "**Stop:** Pause the movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Data Understanding\n",
    "\n",
    "The training data consists of a few hundred videos categorized into one of the above mentioned 5 classes. Each video is around 2-3 seconds long and is divided into 30 frames or images. These videos have been performed by various people performing one of the five gestures infront of a webcam similar to the one mounted on the smart TV.\n",
    "\n",
    "There are two data folders and two csv files. \n",
    "\n",
    "The first data folder *('train')* contains 5 subfolders for each of the gestures where each subfolder represents a video of a gesture. Each subfolder contains 30 frames representing a video. \n",
    "\n",
    "The second data folder *('val')* contains the same as above and is meant to be used for validation purposes.\n",
    "\n",
    "All images in a particular subfolder have the same dimensions but different videos may have different dimensions. Videos have two type of dimensions, either 360x360 or 120x160. The different dimensions are due to using 2 different webcams.\n",
    "\n",
    "The first csv file *('train.csv')* is associated with the train folder. Each row of the CSV file represents one video and contains three main pieces of information. The name of the subfolder which contains 30 images of the video, the name of the gesture, and the numeric label (0-4) of the video. The numeric label suggests different people taking the same video.\n",
    "\n",
    "The second csv file *('val.csv')* is associated with the val folder and follows the same naming structure of its rows as the first csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Importing Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing basic modules\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random seed is set to ensure results dont drastically vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing various keras modules\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation, TimeDistributed\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing csv files\n",
    "train_doc = np.random.permutation(open(r'C:/Users/Avinash Bandlapalli/Desktop/Main Folder/Academics/Post Grad Diploma in Data Science/Courses/Course 6 - Neural Networks and Deep Learning/Module 5 - Gesture Recognition Case Study/Project_data/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open(r'C:/Users/Avinash Bandlapalli/Desktop/Main Folder/Academics/Post Grad Diploma in Data Science/Courses/Course 6 - Neural Networks and Deep Learning/Module 5 - Gesture Recognition Case Study/Project_data/Project_data/val.csv').readlines())\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the csv files which contain the folder names for training and validation.\n",
    "\n",
    "The `batch_size` is initialized at .. to ensure GPU usage at maximum capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "In this sections, we have to normalize and resize images to a specific size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing for preprocessing\n",
    "rows = 120 #X \n",
    "cols = 160 #Y \n",
    "channel = 3 #number of channels in images 3 for color(RGB)\n",
    "frames=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizing all the images, so we can have all the images in a specific size\n",
    "def crop_resize_img(img):\n",
    "    if img.shape[0]!=img.shape[1]:\n",
    "        img = img[0:120,10:150]\n",
    "    resized_image = imresize(img,(rows,cols))\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using percentile to deal with outliers in the data\n",
    "def normalize_image(img):\n",
    "    normalized_image= (img - np.min(img))/(np.max(img)- np.min(img))\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building\n",
    "\n",
    "To analyze videos using neural networks, two types of architectures are commonly used. \n",
    "\n",
    "First is the standard CNN + RNN architecture in which the images of the videos are passed through a CNN which then extracts a feature vector for each image, and then the sequence of the feature vectors are passed through a RNN. The other architecture used is an extension of CNNs - the 3D convolutional network. \n",
    "\n",
    "In this project, both the architectures will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - CNN + RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 - Defining the Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function preprocesses the images and feeds the data to the model in batches\n",
    "def fetch_aug_batchdata(source_path, folder_list, batch_num, batch_size, t,validation):\n",
    "    batch_data = np.zeros((batch_size,frames,rows,cols,channel))\n",
    "    batch_labels = np.zeros((batch_size,5))\n",
    "    batch_data_aug,batch_label_aug = batch_data,batch_labels\n",
    "    batch_data_flip,batch_label_flip = batch_data,batch_labels\n",
    "    img_idx = [x for x in range(0, 30,2)] \n",
    "    for folder in range(batch_size): \n",
    "        imgs = sorted(os.listdir(source_path+'/'+ t[folder + (batch_num*batch_size)].split(';')[0]))\n",
    "        dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "        M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "        for idx, item in enumerate(img_idx):             \n",
    "            image = cv2.imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            resized_image=crop_resize_img(image)\n",
    "            batch_data[folder,idx,:,:,0] = normalize_image(resized_image[:, : , 0])#normalise and feed in the image\n",
    "            batch_data[folder,idx,:,:,1] = normalize_image(resized_image[:, : , 1])#normalise and feed in the image\n",
    "            batch_data[folder,idx,:,:,2] = normalize_image(resized_image[:, : , 2])#normalise and feed in the image\n",
    "            x =resized_image.shape[0]\n",
    "            y =resized_image.shape[1]\n",
    "            batch_data_aug[folder,idx] = (cv2.warpAffine(resized_image, M, (x,y)))\n",
    "            batch_data_flip[folder,idx]= np.flip(resized_image,1)\n",
    "        batch_labels[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        batch_label_aug[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        if int(t[folder + (batch_num * batch_size)].strip().split(';')[2])==0:\n",
    "            batch_label_flip[folder, 1] = 1\n",
    "        elif int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==1:\n",
    "            batch_label_flip[folder, 0] = 1                    \n",
    "        else:\n",
    "            batch_label_flip[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "    batch_data_final = np.append(batch_data, batch_data_aug, axis = 0)\n",
    "    batch_data_final = np.append(batch_data_final, batch_data_flip, axis = 0)\n",
    "    batch_label_final = np.append(batch_labels, batch_label_aug, axis = 0) \n",
    "    batch_label_final = np.append(batch_label_final, batch_label_flip, axis = 0)  \n",
    "    if validation:\n",
    "        batch_data_final=batch_data\n",
    "        batch_label_final= batch_labels       \n",
    "    return batch_data_final,batch_label_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator1(source_path, folder_list, batch_size, validation=False,ablation=None):\n",
    "    print('Source path = ', source_path,'; batch size =',batch_size)\n",
    "    if(ablation!=None):\n",
    "        folder_list=folder_list[:ablation]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            # you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            yield fetch_aug_batchdata(source_path, folder_list, batch, batch_size, t,validation)\n",
    "        # Code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            yield fetch_aug_batchdata(source_path, folder_list, batch, batch_size, t,validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 70\n"
     ]
    }
   ],
   "source": [
    "# calculating number of training sequences, validation sequences, and epochs\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = r'C:/Users/Avinash Bandlapalli/Desktop/Main Folder/Academics/Post Grad Diploma in Data Science/Courses/Course 6 - Neural Networks and Deep Learning/Module 5 - Gesture Recognition Case Study/Project_data/Project_data/train'\n",
    "val_path = r'C:/Users/Avinash Bandlapalli/Desktop/Main Folder/Academics/Post Grad Diploma in Data Science/Courses/Course 6 - Neural Networks and Deep Learning/Module 5 - Gesture Recognition Case Study/Project_data/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 70\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 - Generating and Compling the Model\n",
    "\n",
    "The model uses `TimeDistributed`, `GRU`, and other RNN structures after transfer learning. Last layer is softmax. The network is created to ensure the model is able to fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(120,120,3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "#x.add(Dropout(0.5))\n",
    "features = Dense(64, activation='relu')(x)\n",
    "conv_model = Model(inputs=base_model.input, outputs=features)\n",
    "    \n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "        \n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(conv_model, input_shape=(15,120,120,3)))\n",
    "model.add(GRU(32, return_sequences=True))\n",
    "model.add(GRU(16))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model has been generated. Next step is to compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 15, 64)            15009664  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 15, 32)            9312      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 16)                2352      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 15,021,509\n",
      "Trainable params: 306,821\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# compiling the model and printing summary\n",
    "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the total amount of parameters we have to train.\n",
    "\n",
    "Below the `train_generator` and the `val_generator` will be created to be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and val generators\n",
    "train_generator = generator1(train_path, train_doc, batch_size)\n",
    "val_generator = generator1(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avinash Bandlapalli\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init_conv_lstm' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 - Fitting the Model\n",
    "\n",
    "The model will be fit in this section. Checkpoints will save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "Source path =  /notebooks/storage/Final_data/Project_data/train ; batch size = 16\n",
      "Source path =  /notebooks/storage/Final_data/Project_data/val ; batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/42 [=>............................] - ETA: 1:45 - loss: 1.6539 - categorical_accuracy: 0.2031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 5/42 [==>...........................] - ETA: 1:28 - loss: 1.6864 - categorical_accuracy: 0.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 59s 1s/step - loss: 1.6865 - categorical_accuracy: 0.2181 - val_loss: 1.5986 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00001: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00001-1.68776-0.21719-1.59857-0.28000.h5\n",
      "Epoch 2/70\n",
      "42/42 [==============================] - 39s 929ms/step - loss: 1.6397 - categorical_accuracy: 0.2389 - val_loss: 1.5900 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00002: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00002-1.63751-0.23831-1.58996-0.24000.h5\n",
      "Epoch 3/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.5869 - categorical_accuracy: 0.2486 - val_loss: 1.5622 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00003-1.59086-0.24434-1.56217-0.27000.h5\n",
      "Epoch 4/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.5493 - categorical_accuracy: 0.2985 - val_loss: 1.5530 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00004: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00004-1.54839-0.29864-1.55305-0.30000.h5\n",
      "Epoch 5/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.5380 - categorical_accuracy: 0.3148 - val_loss: 1.5091 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00005: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00005-1.53755-0.31523-1.50906-0.40000.h5\n",
      "Epoch 6/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.4885 - categorical_accuracy: 0.3428 - val_loss: 1.4869 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00006: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00006-1.48738-0.34540-1.48694-0.40000.h5\n",
      "Epoch 7/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.4528 - categorical_accuracy: 0.3684 - val_loss: 1.4412 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00007: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00007-1.45203-0.36953-1.44123-0.40000.h5\n",
      "Epoch 8/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.3837 - categorical_accuracy: 0.4652 - val_loss: 1.4098 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00008: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00008-1.38167-0.46757-1.40982-0.42000.h5\n",
      "Epoch 9/70\n",
      "42/42 [==============================] - 50s 1s/step - loss: 1.3799 - categorical_accuracy: 0.4447 - val_loss: 1.3797 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00009: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00009-1.38107-0.44495-1.37968-0.45000.h5\n",
      "Epoch 10/70\n",
      "42/42 [==============================] - 46s 1s/step - loss: 1.3308 - categorical_accuracy: 0.4715 - val_loss: 1.3218 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00010: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00010-1.33176-0.47210-1.32178-0.51000.h5\n",
      "Epoch 11/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.2733 - categorical_accuracy: 0.4968 - val_loss: 1.3141 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00011: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00011-1.27234-0.49774-1.31409-0.56000.h5\n",
      "Epoch 12/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.2460 - categorical_accuracy: 0.5177 - val_loss: 1.2599 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00012: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00012-1.24632-0.51885-1.25995-0.56000.h5\n",
      "Epoch 13/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.1844 - categorical_accuracy: 0.5821 - val_loss: 1.2813 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00013: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00013-1.18473-0.58220-1.28132-0.53000.h5\n",
      "Epoch 14/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.1552 - categorical_accuracy: 0.5757 - val_loss: 1.2324 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00014: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00014-1.15570-0.57768-1.23242-0.54000.h5\n",
      "Epoch 15/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.1477 - categorical_accuracy: 0.5698 - val_loss: 1.2459 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00015: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00015-1.14729-0.57164-1.24587-0.52000.h5\n",
      "Epoch 16/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.0700 - categorical_accuracy: 0.6327 - val_loss: 1.2236 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00016: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00016-1.07143-0.63348-1.22362-0.57000.h5\n",
      "Epoch 17/70\n",
      "42/42 [==============================] - 45s 1s/step - loss: 1.0722 - categorical_accuracy: 0.6260 - val_loss: 1.1599 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00017: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00017-1.07654-0.62293-1.15986-0.61000.h5\n",
      "Epoch 18/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.0351 - categorical_accuracy: 0.6468 - val_loss: 1.1651 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00018: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00018-1.03591-0.64404-1.16511-0.59000.h5\n",
      "Epoch 19/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.9970 - categorical_accuracy: 0.6476 - val_loss: 1.1732 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00019: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00019-0.99539-0.64857-1.17325-0.58000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 20/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.9467 - categorical_accuracy: 0.6788 - val_loss: 1.1357 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00020: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00020-0.94907-0.68024-1.13571-0.57000.h5\n",
      "Epoch 21/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.9293 - categorical_accuracy: 0.6955 - val_loss: 1.1188 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00021: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00021-0.92312-0.70287-1.11885-0.62000.h5\n",
      "Epoch 22/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.8992 - categorical_accuracy: 0.7272 - val_loss: 1.1081 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00022: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00022-0.89966-0.72549-1.10809-0.58000.h5\n",
      "Epoch 23/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.8700 - categorical_accuracy: 0.7428 - val_loss: 1.1087 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00023: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00023-0.86747-0.74510-1.10866-0.62000.h5\n",
      "Epoch 24/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.8498 - categorical_accuracy: 0.7417 - val_loss: 1.0967 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00024: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00024-0.85053-0.74208-1.09675-0.63000.h5\n",
      "Epoch 25/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.8494 - categorical_accuracy: 0.7529 - val_loss: 1.1134 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00025: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00025-0.85085-0.74962-1.11337-0.61000.h5\n",
      "Epoch 26/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.8199 - categorical_accuracy: 0.7581 - val_loss: 1.0979 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00026: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00026-0.81817-0.75867-1.09791-0.61000.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 27/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.8008 - categorical_accuracy: 0.7838 - val_loss: 1.0921 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00027: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00027-0.80087-0.78281-1.09206-0.61000.h5\n",
      "Epoch 28/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.7922 - categorical_accuracy: 0.7689 - val_loss: 1.0864 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00028: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00028-0.79394-0.76772-1.08643-0.63000.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.7830 - categorical_accuracy: 0.7830 - val_loss: 1.0728 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00029: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00029-0.78113-0.78582-1.07283-0.63000.h5\n",
      "Epoch 30/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.7792 - categorical_accuracy: 0.7942 - val_loss: 1.0800 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00030: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00030-0.78145-0.79336-1.07999-0.65000.h5\n",
      "Epoch 31/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.7499 - categorical_accuracy: 0.8217 - val_loss: 1.0832 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00031: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00031-0.74719-0.82504-1.08317-0.61000.h5\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 32/70\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.7851 - categorical_accuracy: 0.7793 - val_loss: 1.0858 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00032: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00032-0.78666-0.77828-1.08582-0.63000.h5\n",
      "Epoch 33/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.7577 - categorical_accuracy: 0.7972 - val_loss: 1.0812 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00033: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00033-0.75921-0.79638-1.08120-0.62000.h5\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 34/70\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.7540 - categorical_accuracy: 0.8128 - val_loss: 1.0821 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00034: saving model to model_init_conv_lstm_2018-10-0414_07_55.144483/model-00034-0.75076-0.81599-1.08205-0.62000.h5\n",
      "Epoch 35/70\n",
      "27/42 [==================>...........] - ETA: 12s - loss: 0.7274 - categorical_accuracy: 0.8148"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the accuracy of the training set is **81.28%**, while the accuracy of the validation set is **62.0%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Conv3D architecture\n",
    "\n",
    "The next architecture is on another link here: https://github.com/abandlap/Data-Science-Portfolio/blob/master/Post%20Grad%20Diploma/Hand_Gesture_Recognition_TV_project_part2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
